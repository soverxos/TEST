# --- –§–∞–π–ª: cli/backup_unified.py ---
import hashlib
import json
import os
import shutil
import subprocess
import tarfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from typing import Tuple as TypingTuple
from urllib.parse import urlparse

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from .utils import confirm_action, sdb_console

backup_app = typer.Typer(
    name="backup",
    help="üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –±—ç–∫–∞–ø–æ–≤ —Å —Ö–µ—à–∞–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ë–î –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º",
    no_args_is_help=True,
)

console = Console()

# === –ö–û–ù–°–¢–ê–ù–¢–´ ===
DB_BACKUP_DIR_NAME = "database"
FILES_BACKUP_DIR_NAME = "files"
DATA_ARCHIVE_EXTENSION = ".tar.gz"
POSTGRES_BACKUP_FILENAME = "postgres_dump.sql"
MYSQL_BACKUP_FILENAME = "mysql_dump.sql"
USER_CONFIG_DIR_NAME_FOR_BACKUP_DEFAULT = "Config"


# === –£–¢–ò–õ–ò–¢–´ –•–ï–®–ò–†–û–í–ê–ù–ò–Ø ===
def sha256(file_path: Path, chunk_size: int = 65536) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞."""
    h = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


def scan_directory(path: Path, excludes: Optional[List[str]] = None) -> dict[str, str]:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Å–æ–∑–¥–∞–µ—Ç —Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π."""
    import fnmatch
    
    hashes = {}
    excludes = excludes or []
    
    for file in path.rglob("*"):
        if file.is_file():
            rel_path = file.relative_to(path).as_posix()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            excluded = False
            for exclude in excludes:
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å –∑–≤–µ–∑–¥–æ—á–∫–æ–π (*.pyc, *.log –∏ —Ç.–¥.)
                if "*" in exclude:
                    if fnmatch.fnmatch(rel_path, exclude) or fnmatch.fnmatch(file.name, exclude):
                        excluded = True
                        break
                # –ü–æ–ª–Ω—ã–µ –ø—É—Ç–∏ (Data/Cache_data)
                elif "/" in exclude:
                    if rel_path.startswith(exclude) or rel_path == exclude:
                        excluded = True
                        break
                # –ò–º–µ–Ω–∞ –ø–∞–ø–æ–∫ –∏–ª–∏ —Ñ–∞–π–ª–æ–≤
                else:
                    path_parts = rel_path.split("/")
                    if exclude in path_parts or exclude == file.name:
                        excluded = True
                        break
            
            if excluded:
                continue
                
            hashes[rel_path] = sha256(file)
    return hashes


# === –£–¢–ò–õ–ò–¢–´ –ë–î ===
def _get_backup_base_dir() -> Optional[Path]:
    """–ü–æ–ª—É—á–∞–µ—Ç –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±—ç–∫–∞–ø–æ–≤."""
    try:
        project_root = Path(__file__).resolve().parent.parent
        backup_dir = project_root / "backup"
        backup_dir.mkdir(parents=True, exist_ok=True)
        return backup_dir
    except Exception as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—ç–∫–∞–ø–æ–≤: {e}[/]")
        return None


def _find_system_utility(name: str) -> Optional[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—É—é —É—Ç–∏–ª–∏—Ç—É."""
    return shutil.which(name)


def _execute_system_command(
    command: List[str],
    env_vars: Optional[dict] = None,
    input_data: Optional[str] = None,
    show_stdout_on_success: bool = False,
) -> bool:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—É—é –∫–æ–º–∞–Ω–¥—É."""
    full_env = os.environ.copy()
    if env_vars:
        full_env.update(env_vars)

    try:
        result = subprocess.run(
            command,
            env=full_env,
            input=input_data,
            text=True,
            capture_output=True,
            check=True,
        )

        if show_stdout_on_success and result.stdout.strip():
            console.print(result.stdout.strip())

        return True
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã: {e}[/]")
        if e.stderr:
            console.print(f"[red]{e.stderr}[/]")
        return False
    except Exception as e:
        console.print(f"[bold red]–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}[/]")
        return False


# === –û–°–ù–û–í–ù–´–ï –ö–û–ú–ê–ù–î–´ ===


@backup_app.command("create")
def create_backup(
    backup_type: str = typer.Option(
        "full", "--type", "-t", help="–¢–∏–ø –±—ç–∫–∞–ø–∞: full, files, db, custom"
    ),
    dest: Optional[Path] = typer.Option(
        None, "--dest", "-d", help="–ü–∞–ø–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –±—ç–∫–∞–ø–∞"
    ),
    compress: bool = typer.Option(
        True, "--compress/--no-compress", help="–°–∂–∏–º–∞—Ç—å –∞—Ä—Ö–∏–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ)"
    ),
    verify_hashes: bool = typer.Option(
        True,
        "--verify-hashes/--no-verify-hashes",
        help="–°–æ–∑–¥–∞–≤–∞—Ç—å –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤",
    ),
    exclude: Optional[List[str]] = typer.Option(
        None, "--exclude", "-x", help="–ò—Å–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª—ã/–ø–∞–ø–∫–∏"
    ),
    include_data_dirs: Optional[List[str]] = typer.Option(
        None, "--data-dir", "-dd", help=f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ Data –¥–ª—è –±—ç–∫–∞–ø–∞"
    ),
    db_url: Optional[str] = typer.Option(None, "--db-url", help="URL –ë–î –¥–ª—è –±—ç–∫–∞–ø–∞"),
):
    """üöÄ –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –±—ç–∫–∞–ø —Ñ–∞–π–ª–æ–≤ –∏/–∏–ª–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    import time

    start_time = time.time()

    console.print(f"[bold cyan]üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ —Ç–∏–ø–∞: {backup_type.upper()}[/]")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if backup_type == "custom" and dest:
        # –î–ª—è custom –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ª—é–±—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        project_root = Path(__file__).resolve().parent.parent
        source_path = project_root
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        project_root = Path(__file__).resolve().parent.parent
        source_path = project_root

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±–ª–∞—Å—Ç—å –±—ç–∫–∞–ø–∞
    scope_analysis = analyze_backup_scope(source_path)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    for warning in scope_analysis["warnings"]:
        console.print(f"[yellow]{warning}[/]")

    for recommendation in scope_analysis["recommendations"]:
        console.print(f"[blue]{recommendation}[/]")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ –≤–∫–ª—é—á–∞—Ç—å
    include_files = backup_type in ["full", "files", "custom"]
    include_db = backup_type in ["full", "db"]

    # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_base_dir = _get_backup_base_dir()
    if not backup_base_dir:
        return

    # –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    working_name = f"unified_{backup_type}_{timestamp}"
    if compress:
        temp_target = backup_base_dir / f"temp_{working_name}"
        final_target = dest or backup_base_dir / f"{working_name}.tar.gz"
        temp_target.mkdir(parents=True, exist_ok=True)
        working_target = temp_target
    else:
        final_target = dest or backup_base_dir / working_name
        final_target = Path(final_target).expanduser().resolve()
        final_target.mkdir(parents=True, exist_ok=True)
        working_target = final_target

    console.print(f"[dim]üéØ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: {final_target}[/]")

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞
    backup_metadata = {
        "type": f"unified_{backup_type}",
        "timestamp": timestamp,
        "includes_files": include_files,
        "includes_db": include_db,
        "compressed": compress,
        "verify_hashes": verify_hashes,
        "creation_time": datetime.now().isoformat() + "Z",
        "excluded_patterns": [],  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
        "files": {},
        "database": {},
    }

    total_size = 0
    files_count = 0

    # === –≠–¢–ê–ü 1: –§–ê–ô–õ–´ ===
    if include_files:
        console.print(f"\n[cyan]üìÅ –≠—Ç–∞–ø 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤[/]")

        project_root = Path(__file__).resolve().parent.parent

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–∫—ç—à, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã)
        auto_excludes = [
            # Git –∏ VCS
            ".git",
            ".gitignore",
            ".gitattributes",
            ".hg",
            ".svn",
            
            # –í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Python
            ".venv",
            "venv",
            ".env",
            "env",
            "ENV",
            "virtualenv",
            ".virtualenv",
            "pyenv",
            ".python-version",
            
            # Python –∫—ç—à –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            "__pycache__",
            "*.pyc",
            "*.pyo",
            "*.pyd",
            ".Python",
            "*.so",
            ".pytest_cache",
            ".mypy_cache",
            ".coverage",
            ".tox",
            ".nox",
            "htmlcov",
            ".cache",
            "pip-log.txt",
            "pip-delete-this-directory.txt",
            
            # Node.js –∏ JavaScript
            "node_modules",
            "npm-debug.log*",
            "yarn-debug.log*",
            "yarn-error.log*",
            ".npm",
            ".yarn-integrity",
            
            # IDE –∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã
            ".vscode",
            ".idea",
            "*.swp",
            "*.swo",
            "*~",
            ".vim",
            ".emacs.d",
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã
            ".DS_Store",
            ".DS_Store?",
            "._*",
            ".Spotlight-V100",
            ".Trashes",
            "ehthumbs.db",
            "Thumbs.db",
            "Desktop.ini",
            
            # –õ–æ–≥–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            "*.log",
            "logs",
            "*.tmp",
            "*.temp",
            "tmp",
            "temp",
            ".tmp",
            
            # –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤—ã –∏ —Å–±–æ—Ä–∫–∏
            "dist",
            "build",
            "*.egg-info",
            ".eggs",
            "*.egg",
            "*.whl",
            
            # –ö—ç—à –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞
            "Data/Cache_data",
            "Data/Logs",
            "Data/api/cache",
            "Data/monitor/cache",
            
            # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏
            "backup",
            "backups",
            "*.bak",
            "*.backup",
            "temp_*",  # –ò—Å–∫–ª—é—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞–ø–∫–∏ –±—ç–∫–∞–ø–æ–≤
            
            # Docker –∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            ".dockerignore",
            "Dockerfile*",
            "docker-compose*.yml",
            ".docker",
            
            # –ü—Ä–æ—á–∏–µ —Ñ–∞–π–ª—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
            ".env.local",
            ".env.*.local",
            "*.orig",
            ".sass-cache",
            ".parcel-cache",
        ]

        if exclude:
            auto_excludes.extend(exclude)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        backup_metadata["excluded_patterns"] = auto_excludes

        console.print(f"[dim]üö´ –ò—Å–∫–ª—é—á–µ–Ω–∏–π –ø—Ä–∏–º–µ–Ω–µ–Ω–æ: {len(auto_excludes)} ({', '.join(auto_excludes[:5])}...)[/]")

        # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        if verify_hashes:
            console.print("[cyan]üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...[/]")
            file_hashes = scan_directory(project_root, excludes=auto_excludes)
        else:
            console.print("[cyan]üìÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è...[/]")
            import fnmatch
            
            file_hashes = {}
            for file in project_root.rglob("*"):
                if file.is_file():
                    rel_path = file.relative_to(project_root).as_posix()
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É –∏—Å–∫–ª—é—á–µ–Ω–∏–π
                    excluded = False
                    for exclude in auto_excludes:
                        if "*" in exclude:
                            if fnmatch.fnmatch(rel_path, exclude) or fnmatch.fnmatch(file.name, exclude):
                                excluded = True
                                break
                        elif "/" in exclude:
                            if rel_path.startswith(exclude) or rel_path == exclude:
                                excluded = True
                                break
                        else:
                            path_parts = rel_path.split("/")
                            if exclude in path_parts or exclude == file.name:
                                excluded = True
                                break
                    
                    if not excluded:
                        file_hashes[rel_path] = ""  # –ü—É—Å—Ç–æ–π —Ö–µ—à

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        for rel_path in file_hashes.keys():
            file_path = project_root / rel_path
            if file_path.exists():
                total_size += file_path.stat().st_size

        files_count = len(file_hashes)
        console.print(f"   üìÅ –§–∞–π–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {files_count:,}")
        console.print(f"   üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤: {total_size / (1024*1024):.1f}MB")

        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        files_dir = working_target / FILES_BACKUP_DIR_NAME
        files_dir.mkdir(parents=True, exist_ok=True)

        console.print("[cyan]üì¶ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...[/]")
        for rel_path in file_hashes:
            src = project_root / rel_path
            dest_path = files_dir / rel_path
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dest_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤
        if verify_hashes:
            with open(working_target / "file_hashes.json", "w", encoding="utf-8") as f:
                json.dump(file_hashes, f, indent=2)

        backup_metadata["files"] = {
            "count": files_count,
            "total_size": total_size,
            "hashes_enabled": verify_hashes,
        }

    # === –≠–¢–ê–ü 2: –ë–ê–ó–ê –î–ê–ù–ù–´–• ===
    if include_db:
        console.print(f"\n[cyan]üóÉÔ∏è  –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö[/]")

        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º —Ç–∏–ø –ë–î
        db_info = detect_database_type()
        console.print(f"[cyan]üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–∏–ø –ë–î: {db_info['type'].upper()}[/]")

        db_dir = working_target / DB_BACKUP_DIR_NAME
        db_dir.mkdir(parents=True, exist_ok=True)

        db_success = False

        if db_info["type"] == "sqlite":
            console.print(
                "[cyan]üìÑ SQLite –æ–±–Ω–∞—Ä—É–∂–µ–Ω - —Ñ–∞–π–ª—ã —É–∂–µ –≤–∫–ª—é—á–µ–Ω—ã –≤ –±—ç–∫–∞–ø —Ñ–∞–π–ª–æ–≤[/]"
            )
            if db_info["detected_files"]:
                console.print(
                    f"[green]‚úÖ –ù–∞–π–¥–µ–Ω—ã SQLite —Ñ–∞–π–ª—ã: {', '.join(db_info['detected_files'])}[/]"
                )

            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
            sqlite_info = {
                "type": "sqlite",
                "files": db_info["detected_files"],
                "note": "SQLite files are included in files backup",
                "location": db_info["location"],
            }
            with open(db_dir / "sqlite_info.json", "w", encoding="utf-8") as f:
                json.dump(sqlite_info, f, indent=2)

            db_success = True
            backup_metadata["database"] = {
                "type": "sqlite",
                "success": True,
                "files": db_info["detected_files"],
                "backup_method": "included_in_files",
            }

        elif db_info["type"] == "postgresql":
            console.print("[cyan]üêò –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ PostgreSQL...[/]")
            pg_dump = _find_system_utility("pg_dump")
            if pg_dump and db_info["location"]:
                dump_file = db_dir / POSTGRES_BACKUP_FILENAME
                cmd = [pg_dump, db_info["location"], "-f", str(dump_file)]
                db_success = _execute_system_command(cmd)
                if db_success:
                    console.print("[green]‚úÖ PostgreSQL –±—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω[/]")
            else:
                console.print("[red]‚ùå pg_dump –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ URL –ë–î –Ω–µ —É–∫–∞–∑–∞–Ω[/]")

        elif db_info["type"] == "mysql":
            console.print("[cyan]üê¨ –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ MySQL...[/]")
            mysqldump = _find_system_utility("mysqldump")
            if mysqldump and db_info["location"]:
                dump_file = db_dir / MYSQL_BACKUP_FILENAME
                # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞, –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å URL
                console.print(
                    "[yellow]‚ö†Ô∏è MySQL –±—ç–∫–∞–ø —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏[/]"
                )
            else:
                console.print("[red]‚ùå mysqldump –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ URL –ë–î –Ω–µ —É–∫–∞–∑–∞–Ω[/]")

        elif db_info["type"] == "none":
            console.print("[yellow]‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞[/]")
            console.print(
                "[dim]üí° –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SQLite, —Ñ–∞–π–ª—ã –ë–î –±—É–¥—É—Ç –≤–∫–ª—é—á–µ–Ω—ã –≤ –±—ç–∫–∞–ø —Ñ–∞–π–ª–æ–≤[/]"
            )

        else:
            console.print(f"[yellow]‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ë–î: {db_info['type']}[/]")

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ë–î
        if "database" not in backup_metadata:
            backup_metadata["database"] = {
                "type": db_info["type"],
                "success": db_success,
                "needs_separate_backup": db_info["needs_separate_backup"],
                "detected_files": db_info["detected_files"],
            }

    # === –≠–¢–ê–ü 3: –ú–ï–¢–ê–î–ê–ù–ù–´–ï ===
    console.print(f"\n[cyan]üìã –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö[/]")

    with open(working_target / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(backup_metadata, f, indent=2, ensure_ascii=False)

    # === –≠–¢–ê–ü 4: –ê–†–•–ò–í–ê–¶–ò–Ø ===
    if compress:
        console.print(f"\n[cyan]üóúÔ∏è –≠—Ç–∞–ø 4: –ê—Ä—Ö–∏–≤–∞—Ü–∏—è[/]")
        final_target = Path(final_target).expanduser().resolve()

        with tarfile.open(final_target, "w:gz") as tar:
            for item in working_target.rglob("*"):
                if item.is_file():
                    arcname = item.relative_to(working_target)
                    tar.add(item, arcname=arcname)

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
        shutil.rmtree(working_target)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∂–∞—Ç–∏—è
        archive_size = final_target.stat().st_size
        compression_ratio = (
            (1 - archive_size / total_size) * 100 if total_size > 0 else 0
        )

        console.print(f"   üì¶ –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {final_target.name}")
        console.print(f"   üìä –†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {archive_size / (1024*1024):.1f}MB")
        console.print(f"   üìà –°–∂–∞—Ç–∏–µ: {compression_ratio:.1f}%")

    # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
    end_time = time.time()
    duration = end_time - start_time

    console.print(f"\n[green]üéâ –ë—ç–∫–∞–ø –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ![/]")
    console.print(
        Panel.fit(
            f"[green]‚úÖ –¢–∏–ø:[/] {backup_type.upper()}\n"
            f"[green]üìÅ –§–∞–π–ª–æ–≤:[/] {files_count:,} ({total_size / (1024*1024):.1f}MB)\n"
            f"[green]üóÉÔ∏è –ë–î:[/] {'–≤–∫–ª—é—á–µ–Ω–∞' if include_db else '–Ω–µ –≤–∫–ª—é—á–µ–Ω–∞'}\n"
            f"[green]üîç –•–µ—à–∏:[/] {'–≤–∫–ª—é—á–µ–Ω—ã' if verify_hashes else '–æ—Ç–∫–ª—é—á–µ–Ω—ã'}\n"
            f"[green]‚è±Ô∏è –í—Ä–µ–º—è:[/] {duration:.1f}—Å\n"
            f"[green]üì¶ –†–µ–∑—É–ª—å—Ç–∞—Ç:[/] {final_target}",
            title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±—ç–∫–∞–ø–∞",
            border_style="green",
        )
    )


@backup_app.command("list")
def list_backups(
    backup_type: str = typer.Option(
        "all", "--type", "-t", help="–¢–∏–ø –±—ç–∫–∞–ø–æ–≤ –¥–ª—è –ø–æ–∫–∞–∑–∞: all, files, db, unified"
    ),
    show_hashes: bool = typer.Option(
        False, "--show-hashes", help="–ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ö–µ—à–∞—Ö"
    ),
):
    """üìã –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤."""
    backup_base_dir = _get_backup_base_dir()
    if not backup_base_dir:
        return

    console.print("[bold cyan]üìã –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤[/]")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –±—ç–∫–∞–ø—ã
    backups = []

    # –ü–æ–∏—Å–∫ –∞—Ä—Ö–∏–≤–æ–≤ .tar.gz
    for backup_file in backup_base_dir.glob("*.tar.gz"):
        try:
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with tarfile.open(backup_file, "r:gz") as tar:
                try:
                    metadata_member = tar.getmember("metadata.json")
                    metadata_file = tar.extractfile(metadata_member)
                    metadata = json.load(metadata_file)

                    backup_info = {
                        "name": backup_file.name,
                        "path": backup_file,
                        "type": metadata.get("type", "unknown"),
                        "timestamp": metadata.get("timestamp", "unknown"),
                        "size": backup_file.stat().st_size,
                        "includes_files": metadata.get("includes_files", False),
                        "includes_db": metadata.get("includes_db", False),
                        "compressed": True,
                        "verify_hashes": metadata.get("verify_hashes", False),
                    }
                    backups.append(backup_info)
                except:
                    # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –±—ç–∫–∞–ø–∞
                    backup_info = {
                        "name": backup_file.name,
                        "path": backup_file,
                        "type": "legacy",
                        "timestamp": "unknown",
                        "size": backup_file.stat().st_size,
                        "includes_files": True,
                        "includes_db": False,
                        "compressed": True,
                        "verify_hashes": False,
                    }
                    backups.append(backup_info)
        except:
            continue

    # –ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ —Å –±—ç–∫–∞–ø–∞–º–∏
    for backup_dir in backup_base_dir.iterdir():
        if backup_dir.is_dir() and not backup_dir.name.startswith("temp_"):
            metadata_file = backup_dir / "metadata.json"
            if metadata_file.exists():
                try:
                    with open(metadata_file, encoding="utf-8") as f:
                        metadata = json.load(f)

                    backup_info = {
                        "name": backup_dir.name,
                        "path": backup_dir,
                        "type": metadata.get("type", "unknown"),
                        "timestamp": metadata.get("timestamp", "unknown"),
                        "size": sum(
                            f.stat().st_size
                            for f in backup_dir.rglob("*")
                            if f.is_file()
                        ),
                        "includes_files": metadata.get("includes_files", False),
                        "includes_db": metadata.get("includes_db", False),
                        "compressed": False,
                        "verify_hashes": metadata.get("verify_hashes", False),
                    }
                    backups.append(backup_info)
                except:
                    continue

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
    if backup_type != "all":
        backups = [b for b in backups if backup_type in b["type"]]

    if not backups:
        console.print("[yellow]üì≠ –ë—ç–∫–∞–ø—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã[/]")
        return

    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    table = Table(title="üíæ –î–æ—Å—Ç—É–ø–Ω—ã–µ –±—ç–∫–∞–ø—ã")
    table.add_column("–ò–º—è", style="cyan")
    table.add_column("–¢–∏–ø", style="magenta")
    table.add_column("–†–∞–∑–º–µ—Ä", style="green")
    table.add_column("–§–∞–π–ª—ã", style="blue")
    table.add_column("–ë–î", style="red")
    if show_hashes:
        table.add_column("–•–µ—à–∏", style="yellow")
    table.add_column("–î–∞—Ç–∞", style="dim")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    backups.sort(key=lambda x: x["timestamp"], reverse=True)

    for backup in backups:
        size_mb = backup["size"] / (1024 * 1024)
        files_icon = "‚úÖ" if backup["includes_files"] else "‚ùå"
        db_icon = "‚úÖ" if backup["includes_db"] else "‚ùå"
        hashes_icon = "üîç" if backup["verify_hashes"] else "‚ùå"

        row = [backup["name"], backup["type"], f"{size_mb:.1f}MB", files_icon, db_icon]

        if show_hashes:
            row.append(hashes_icon)

        row.append(backup["timestamp"])
        table.add_row(*row)

    console.print(table)
    console.print(f"\n[dim]–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(backups)} –±—ç–∫–∞–ø–æ–≤[/]")


@backup_app.command("restore")
def restore_backup(
    backup_path: str = typer.Argument(..., help="–ò–º—è –∏–ª–∏ –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"),
    dest: Path = typer.Argument(..., help="–¶–µ–ª–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"),
    backup_type: str = typer.Option(
        "auto", "--type", "-t", help="–ß—Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å: auto, files, db, all"
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="–ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
    ),
    verify_hashes: bool = typer.Option(
        True,
        "--verify-hashes/--no-verify-hashes",
        help="–ü—Ä–æ–≤–µ—Ä—è—Ç—å —Ö–µ—à–∏ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏",
    ),
    skip_on_error: bool = typer.Option(
        False, "--skip-on-error", help="–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"
    ),
    restore_db: bool = typer.Option(
        True, "--db/--no-db", help="–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"
    ),
    db_url: Optional[str] = typer.Option(
        None, "--db-url", help="URL –ë–î –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"
    ),
):
    """üì• –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ñ–∞–π–ª—ã –∏/–∏–ª–∏ –ë–î –∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞."""
    backup_path = _resolve_backup_path(backup_path)
    console.print(f"[bold cyan]üì• –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑: {backup_path}[/]")

    if dry_run:
        console.print(
            "[bold yellow]üîç –°–£–•–û–ô –ó–ê–ü–£–°–ö: –ü–æ–∫–∞–∑—ã–≤–∞—é —á—Ç–æ –±—É–¥–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ[/]"
        )
    else:
        console.print(f"[bold cyan]üì• –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑: {backup_path}[/]")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±—ç–∫–∞–ø–∞
    is_archive = backup_path.suffix == ".gz" and backup_path.suffixes[-2:] == [
        ".tar",
        ".gz",
    ]

    if is_archive:
        console.print("[cyan]üì¶ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–∂–∞—Ç—ã–π –∞—Ä—Ö–∏–≤[/]")
        temp_dir = (
            backup_path.parent
            / f"temp_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        temp_dir.mkdir(exist_ok=True)

        try:
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(temp_dir)
            restore_source = temp_dir
            cleanup_temp = True
        except Exception as e:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}[/]")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return
    else:
        console.print("[cyan]üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å –±—ç–∫–∞–ø–æ–º[/]")
        restore_source = backup_path
        cleanup_temp = False

    try:
        # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_file = restore_source / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, encoding="utf-8") as f:
                metadata = json.load(f)
        else:
            console.print(
                "[yellow]‚ö†Ô∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ä–µ–∂–∏–º[/]"
            )
            metadata = {
                "includes_files": True,
                "includes_db": False,
                "verify_hashes": False,
            }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å
        should_restore_files = backup_type in ["auto", "files", "all"] and metadata.get(
            "includes_files", False
        )
        should_restore_db = (
            backup_type in ["auto", "db", "all"]
            and metadata.get("includes_db", False)
            and restore_db
        )

        console.print(f"[cyan]üìã –ü–ª–∞–Ω –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è:[/]")
        console.print(f"   üìÅ –§–∞–π–ª—ã: {'‚úÖ' if should_restore_files else '‚ùå'}")
        console.print(f"   üóÉÔ∏è –ë–î: {'‚úÖ' if should_restore_db else '‚ùå'}")

        if not should_restore_files and not should_restore_db:
            console.print("[red]‚ùå –ù–µ—á–µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å![/]")
            return

        if not dry_run:
            dest.mkdir(parents=True, exist_ok=True)

        restored_files = 0
        skipped_files = 0
        hash_errors = []

        # === –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –§–ê–ô–õ–û–í ===
        if should_restore_files:
            console.print(f"\n[cyan]üìÅ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤[/]")

            files_dir = restore_source / FILES_BACKUP_DIR_NAME
            if files_dir.exists():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–µ—à–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                file_hashes = {}
                hash_file = restore_source / "file_hashes.json"
                if verify_hashes and hash_file.exists():
                    with open(hash_file, encoding="utf-8") as f:
                        file_hashes = json.load(f)
                    console.print(f"[cyan]üîç –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ö–µ—à–µ–π: {len(file_hashes)}[/]")

                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã
                for src_file in files_dir.rglob("*"):
                    if src_file.is_file():
                        rel_path = src_file.relative_to(files_dir).as_posix()
                        dest_file = dest / rel_path

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–µ—à –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if verify_hashes and rel_path in file_hashes:
                            expected_hash = file_hashes[rel_path]
                            if expected_hash:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ö–µ—à–∏
                                actual_hash = sha256(src_file)
                                if actual_hash != expected_hash:
                                    error_msg = f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ö–µ—à: {rel_path}"
                                    hash_errors.append(error_msg)

                                    if skip_on_error:
                                        console.print(
                                            f"[yellow]{error_msg} (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)[/]"
                                        )
                                        skipped_files += 1
                                        continue
                                    else:
                                        console.print(f"[red]{error_msg}[/]")
                                        console.print(
                                            f"[red]‚ùå –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ![/]"
                                        )
                                        return

                        if dry_run:
                            console.print(f"  [cyan]‚Üí[/] {rel_path}")
                        else:
                            dest_file.parent.mkdir(parents=True, exist_ok=True)
                            shutil.copy2(src_file, dest_file)
                            restored_files += 1

                if not dry_run:
                    console.print(
                        f"[green]‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {restored_files}[/]"
                    )
            else:
                console.print("[yellow]‚ö†Ô∏è –§–∞–π–ª—ã –≤ –±—ç–∫–∞–ø–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã[/]")

        # === –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ë–î ===
        if should_restore_db and not dry_run:
            console.print(f"\n[cyan]üóÉÔ∏è –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö[/]")

            db_dir = restore_source / DB_BACKUP_DIR_NAME
            if db_dir.exists():
                db_info = metadata.get("database", {})
                db_type = db_info.get("type", "unknown")

                if db_type == "postgresql":
                    pg_restore = _find_system_utility("psql")
                    dump_file = db_dir / POSTGRES_BACKUP_FILENAME
                    if pg_restore and dump_file.exists():
                        console.print("[cyan]üêò –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ PostgreSQL...[/]")
                        cmd = [pg_restore, db_url, "-f", str(dump_file)]
                        if _execute_system_command(cmd):
                            console.print("[green]‚úÖ –ë–î PostgreSQL –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞[/]")
                        else:
                            console.print("[red]‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è PostgreSQL[/]")

                elif db_type == "mysql":
                    mysql = _find_system_utility("mysql")
                    dump_file = db_dir / MYSQL_BACKUP_FILENAME
                    if mysql and dump_file.exists():
                        console.print("[cyan]üê¨ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ MySQL...[/]")
                        # –ö–æ–º–∞–Ω–¥—É –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ë–î
                        console.print(
                            "[yellow]‚ö†Ô∏è –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ MySQL —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏[/]"
                        )
            else:
                console.print("[yellow]‚ö†Ô∏è –ë–î –≤ –±—ç–∫–∞–ø–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞[/]")

        # === –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
        if dry_run:
            console.print(f"\n[yellow]üîç –°—É—Ö–æ–π –∑–∞–ø—É—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω[/]")
        else:
            console.print(f"\n[green]üéâ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ![/]")
            if verify_hashes and hash_errors:
                console.print(f"[red]‚ö†Ô∏è –û—à–∏–±–æ–∫ —Ö–µ—à–µ–π: {len(hash_errors)}[/]")
                if skipped_files:
                    console.print(f"[yellow]‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {skipped_files}[/]")
            elif verify_hashes:
                console.print("[green]üîç –í—Å–µ —Ö–µ—à–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã[/]")

    finally:
        if cleanup_temp:
            shutil.rmtree(temp_dir, ignore_errors=True)


@backup_app.command("verify")
def verify_backup(
    backup_path: str = typer.Argument(..., help="–ò–º—è –∏–ª–∏ –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"),
    checksum: bool = typer.Option(
        True, "--checksum/--no-checksum", help="–ü—Ä–æ–≤–µ—Ä—è—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã —Ñ–∞–π–ª–æ–≤"
    ),
):
    """üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞."""
    backup_path = _resolve_backup_path(backup_path)
    console.print(f"[bold cyan]üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ç–∫–∞–ø–∞: {backup_path}[/]")

    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ restore - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    is_archive = backup_path.suffix == ".gz" and backup_path.suffixes[-2:] == [
        ".tar",
        ".gz",
    ]

    if is_archive:
        console.print("[cyan]üì¶ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞[/]")
        temp_dir = (
            backup_path.parent
            / f"temp_verify_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        temp_dir.mkdir(exist_ok=True)

        try:
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(temp_dir)
            verify_source = temp_dir
            cleanup_temp = True
        except Exception as e:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ: {e}[/]")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return
    else:
        verify_source = backup_path
        cleanup_temp = False

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_file = verify_source / "metadata.json"
        if not metadata_file.exists():
            console.print("[red]‚ùå –§–∞–π–ª metadata.json –Ω–µ –Ω–∞–π–¥–µ–Ω[/]")
            return

        with open(metadata_file, encoding="utf-8") as f:
            metadata = json.load(f)

        console.print(f"[green]üìã –¢–∏–ø –±—ç–∫–∞–ø–∞: {metadata.get('type', 'unknown')}[/]")

        errors = []
        total_files = 0
        verified_files = 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã
        if metadata.get("includes_files", False):
            console.print(f"[cyan]üìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤...[/]")

            files_dir = verify_source / FILES_BACKUP_DIR_NAME
            if files_dir.exists():
                hash_file = verify_source / "file_hashes.json"
                if checksum and hash_file.exists():
                    with open(hash_file, encoding="utf-8") as f:
                        file_hashes = json.load(f)

                    console.print(
                        f"[cyan]üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ {len(file_hashes)} —Ñ–∞–π–ª–æ–≤ —Å —Ö–µ—à–∞–º–∏...[/]"
                    )

                    for rel_path, expected_hash in file_hashes.items():
                        total_files += 1
                        file_path = files_dir / rel_path

                        if not file_path.exists():
                            errors.append(f"–§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {rel_path}")
                            continue

                        if expected_hash:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Ö–µ—à–∏
                            actual_hash = sha256(file_path)
                            if actual_hash != expected_hash:
                                errors.append(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ö–µ—à: {rel_path}")
                                continue

                        verified_files += 1
                else:
                    console.print(
                        "[yellow]‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–µ—à–µ–π –æ—Ç–∫–ª—é—á–µ–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞[/]"
                    )
                    for file_path in files_dir.rglob("*"):
                        if file_path.is_file():
                            total_files += 1
                            verified_files += 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ë–î
        if metadata.get("includes_db", False):
            console.print(f"[cyan]üóÉÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...[/]")

            db_dir = verify_source / DB_BACKUP_DIR_NAME
            if db_dir.exists():
                db_info = metadata.get("database", {})
                expected_file = db_info.get("backup_file", "")
                if expected_file:
                    db_file = db_dir / expected_file
                    if db_file.exists():
                        console.print(f"[green]‚úÖ –§–∞–π–ª –ë–î –Ω–∞–π–¥–µ–Ω: {expected_file}[/]")
                    else:
                        errors.append(f"–§–∞–π–ª –ë–î –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {expected_file}")

        # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if errors:
            console.print(f"\n[red]‚ùå –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: {len(errors)}[/]")
            for error in errors[:10]:
                console.print(f"  ‚Ä¢ {error}")
            if len(errors) > 10:
                console.print(f"  ‚Ä¢ ... –∏ –µ—â—ë {len(errors) - 10} –æ—à–∏–±–æ–∫")
        else:
            console.print(f"\n[green]‚úÖ –ë—ç–∫–∞–ø —Ü–µ–ª–æ—Å—Ç–µ–Ω![/]")
            console.print(f"   üìÅ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {verified_files}/{total_files}")
            if checksum:
                console.print(f"   üîç –í—Å–µ —Ö–µ—à–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")

    finally:
        if cleanup_temp:
            shutil.rmtree(temp_dir, ignore_errors=True)


@backup_app.command("info")
def backup_info(
    backup_path: str = typer.Argument(
        ..., help="–ò–º—è –∏–ª–∏ –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    )
):
    """üìä –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±—ç–∫–∞–ø–µ."""
    backup_path = _resolve_backup_path(backup_path)
    console.print(f"[bold cyan]üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—ç–∫–∞–ø–µ: {backup_path}[/]")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±—ç–∫–∞–ø–∞ –∏ —á–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º –∫–æ–º–∞–Ω–¥–∞–º
    is_archive = backup_path.suffix == ".gz" and backup_path.suffixes[-2:] == [
        ".tar",
        ".gz",
    ]

    if is_archive:
        archive_size = backup_path.stat().st_size
        console.print(
            f"[cyan]üì¶ –¢–∏–ø: –°–∂–∞—Ç—ã–π –∞—Ä—Ö–∏–≤ ({archive_size / (1024*1024):.1f}MB)[/]"
        )

        temp_dir = (
            backup_path.parent / f"temp_info_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        temp_dir.mkdir(exist_ok=True)

        try:
            with tarfile.open(backup_path, "r:gz") as tar:
                metadata_files = ["metadata.json"]
                for member in tar.getmembers():
                    if member.name in metadata_files:
                        tar.extract(member, temp_dir)

            info_source = temp_dir
            cleanup_temp = True
        except Exception as e:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {e}[/]")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return
    else:
        console.print("[cyan]üìÅ –¢–∏–ø: –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏[/]")
        info_source = backup_path
        cleanup_temp = False

    try:
        metadata_file = info_source / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, encoding="utf-8") as f:
                metadata = json.load(f)

            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            console.print(
                Panel.fit(
                    f"[green]üè∑Ô∏è –¢–∏–ø:[/] {metadata.get('type', 'unknown')}\n"
                    f"[green]üìÖ –°–æ–∑–¥–∞–Ω:[/] {metadata.get('timestamp', 'unknown')}\n"
                    f"[green]üìÅ –í–∫–ª—é—á–∞–µ—Ç —Ñ–∞–π–ª—ã:[/] {'‚úÖ' if metadata.get('includes_files') else '‚ùå'}\n"
                    f"[green]üóÉÔ∏è –í–∫–ª—é—á–∞–µ—Ç –ë–î:[/] {'‚úÖ' if metadata.get('includes_db') else '‚ùå'}\n"
                    f"[green]üîç –•–µ—à–∏:[/] {'‚úÖ' if metadata.get('verify_hashes') else '‚ùå'}\n"
                    f"[green]üóúÔ∏è –°–∂–∞—Ç–∏–µ:[/] {'‚úÖ' if metadata.get('compressed') else '‚ùå'}",
                    title="üìã –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                    border_style="green",
                )
            )

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–∞—Ö
            files_info = metadata.get("files", {})
            if files_info:
                console.print(
                    Panel.fit(
                        f"[blue]üìÅ –§–∞–π–ª–æ–≤:[/] {files_info.get('count', 0):,}\n"
                        f"[blue]üìä –†–∞–∑–º–µ—Ä:[/] {files_info.get('total_size', 0) / (1024*1024):.1f}MB\n"
                        f"[blue]üîç –•–µ—à–∏:[/] {'–≤–∫–ª—é—á–µ–Ω—ã' if files_info.get('hashes_enabled') else '–æ—Ç–∫–ª—é—á–µ–Ω—ã'}",
                        title="üìÅ –§–∞–π–ª—ã",
                        border_style="blue",
                    )
                )

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ë–î
            db_info = metadata.get("database", {})
            if db_info and db_info.get("type") != "unknown":
                console.print(
                    Panel.fit(
                        f"[red]üóÉÔ∏è –¢–∏–ø –ë–î:[/] {db_info.get('type', 'unknown')}\n"
                        f"[red]‚úÖ –£—Å–ø–µ—à–Ω–æ:[/] {'‚úÖ' if db_info.get('success') else '‚ùå'}\n"
                        f"[red]üìÑ –§–∞–π–ª:[/] {db_info.get('backup_file', '–Ω–µ —É–∫–∞–∑–∞–Ω')}",
                        title="üóÉÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö",
                        border_style="red",
                    )
                )

            console.print("[green]‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ[/]")
        else:
            console.print("[red]‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã[/]")

    finally:
        if cleanup_temp:
            shutil.rmtree(temp_dir, ignore_errors=True)


# === –£–õ–£–ß–®–ï–ù–ù–´–ï –£–¢–ò–õ–ò–¢–´ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø ===


def detect_database_type(path: Optional[Path] = None) -> Dict[str, Any]:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ–µ–∫—Ç–µ."""

    if path is None:
        # –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ (–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è sdb.py)
        search_path = Path(__file__).resolve().parent.parent
    else:
        search_path = path.resolve()

    # –ü–æ–∏—Å–∫ SQLite —Ñ–∞–π–ª–æ–≤
    sqlite_files = []
    for pattern in ["**/*.db", "**/*.sqlite", "**/*.sqlite3"]:
        sqlite_files.extend(search_path.glob(pattern))

    # –ò—Å–∫–ª—é—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    sqlite_files = [
        f
        for f in sqlite_files
        if not any(
            exclude in str(f)
            for exclude in [".git", "__pycache__", ".pytest_cache", ".venv"]
        )
    ]

    if sqlite_files:
        return {
            "type": "sqlite",
            "location": str(sqlite_files[0]),  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
            "needs_separate_backup": False,
            "detected_files": [str(f.relative_to(search_path)) for f in sqlite_files],
        }

    # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –ë–î
    config_files = [
        search_path / ".env",
        search_path / ".env.local",
        search_path / "config.py",
        search_path / "settings.py",
    ]

    for config_file in config_files:
        if config_file.exists():
            content = config_file.read_text()

            # –ü–æ–∏—Å–∫ PostgreSQL
            if any(
                keyword in content.lower()
                for keyword in ["postgresql", "postgres", "psycopg"]
            ):
                return {
                    "type": "postgresql",
                    "location": "external_server",
                    "needs_separate_backup": True,
                    "config_file": str(config_file),
                    "detected_files": [],
                }

            # –ü–æ–∏—Å–∫ MySQL
            if any(
                keyword in content.lower()
                for keyword in ["mysql", "pymysql", "mysqlclient"]
            ):
                return {
                    "type": "mysql",
                    "location": "external_server",
                    "needs_separate_backup": True,
                    "config_file": str(config_file),
                    "detected_files": [],
                }

    return {
        "type": "none",
        "location": "not_found",
        "needs_separate_backup": False,
        "detected_files": [],
    }


def analyze_backup_scope(source_path: Path) -> dict:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±–ª–∞—Å—Ç—å –±—ç–∫–∞–ø–∞ –∏ –≤—ã–¥–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    project_root = Path(__file__).resolve().parent.parent
    analysis = {
        "is_project_root": False,
        "is_external_path": False,
        "includes_project_files": False,
        "missing_important_dirs": [],
        "recommendations": [],
        "warnings": [],
    }

    source_path = source_path.resolve()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—É—Ç—å –∫–æ—Ä–Ω–µ–º –ø—Ä–æ–µ–∫—Ç–∞
    if source_path == project_root:
        analysis["is_project_root"] = True
        analysis["includes_project_files"] = True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø—É—Ç—å –≤–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
    try:
        source_path.relative_to(project_root)
        analysis["is_external_path"] = False
    except ValueError:
        analysis["is_external_path"] = True
        analysis["warnings"].append(
            "‚ö†Ô∏è –£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –í–ù–ï –ø—Ä–æ–µ–∫—Ç–∞ SwiftDevBot"
        )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
    important_dirs = [
        ("Systems/core", "–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –±–æ—Ç–∞"),
        ("Systems/cli", "CLI –∫–æ–º–∞–Ω–¥—ã"),
        ("modules", "–ú–æ–¥—É–ª–∏ –±–æ—Ç–∞"),
        ("Data", "–î–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç–∞"),
        ("Systems/locales", "–õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è"),
    ]

    for dir_name, description in important_dirs:
        dir_path = project_root / dir_name
        if dir_path.exists():
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–∞–µ—Ç—Å—è –ª–∏ —ç—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤ –±—ç–∫–∞–ø
                source_path.relative_to(dir_path.parent)
                if not any(source_path == parent for parent in dir_path.parents):
                    analysis["missing_important_dirs"].append(
                        f"{dir_name} ({description})"
                    )
            except ValueError:
                analysis["missing_important_dirs"].append(f"{dir_name} ({description})")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if analysis["is_external_path"]:
        analysis["recommendations"].append(
            "üí° –î–ª—è –±—ç–∫–∞–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞ SwiftDevBot –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞"
        )

    if analysis["missing_important_dirs"]:
        analysis["warnings"].append(
            f"‚ö†Ô∏è –ù–µ –≤–∫–ª—é—á–µ–Ω—ã –≤–∞–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {', '.join(analysis['missing_important_dirs'][:3])}"
        )
        analysis["recommendations"].append(
            "üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±—ç–∫–∞–ø–∞ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞: --type=full –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –ø—É—Ç–∏"
        )

    return analysis


@backup_app.command("check")
def check_project_config(
    path: Optional[Path] = typer.Argument(
        None, help="–ü—É—Ç—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞)"
    )
):
    """üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±—ç–∫–∞–ø—É."""

    if path is None:
        path = Path(__file__).resolve().parent.parent
    else:
        path = path.expanduser().resolve()

    console.print(f"[bold cyan]üîç –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞: {path}[/]")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±–ª–∞—Å—Ç—å
    scope_analysis = analyze_backup_scope(path)

    # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –ë–î –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—É—Ç–∏
    db_info = detect_database_type(path)

    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    console.print(
        Panel.fit(
            f"[green]üìç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –ø—É—Ç—å:[/] {path}\n"
            f"[green]üè† –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞:[/] {'‚úÖ' if scope_analysis['is_project_root'] else '‚ùå'}\n"
            f"[green]üåê –í–Ω–µ—à–Ω–∏–π –ø—É—Ç—å:[/] {'‚ö†Ô∏è' if scope_analysis['is_external_path'] else '‚úÖ'}\n"
            f"[green]üìÅ –í–∫–ª—é—á–∞–µ—Ç —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞:[/] {'‚úÖ' if scope_analysis['includes_project_files'] else '‚ùå'}",
            title="üìã –ê–Ω–∞–ª–∏–∑ –æ–±–ª–∞—Å—Ç–∏ –±—ç–∫–∞–ø–∞",
            border_style="blue",
        )
    )

    console.print(
        Panel.fit(
            f"[green]üóÉÔ∏è –¢–∏–ø –ë–î:[/] {db_info['type'].upper()}\n"
            f"[green]üìç –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:[/] {db_info['location'] or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
            f"[green]üîÑ –ù—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–π –±—ç–∫–∞–ø:[/] {'‚úÖ' if db_info['needs_separate_backup'] else '‚ùå'}\n"
            f"[green]üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ñ–∞–π–ª—ã:[/] {', '.join(db_info['detected_files']) if db_info['detected_files'] else '–Ω–µ—Ç'}",
            title="üóÉÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ë–î",
            border_style="green",
        )
    )

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    if scope_analysis["warnings"]:
        console.print("\n[bold red]‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:[/]")
        for warning in scope_analysis["warnings"]:
            console.print(f"  {warning}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if scope_analysis["recommendations"]:
        console.print("\n[bold blue]üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:[/]")
        for recommendation in scope_analysis["recommendations"]:
            console.print(f"  {recommendation}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±—ç–∫–∞–ø—É
    console.print(f"\n[bold cyan]üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–º–∞–Ω–¥—ã –±—ç–∫–∞–ø–∞:[/]")

    if scope_analysis["is_project_root"]:
        console.print("  [green]# –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø –ø—Ä–æ–µ–∫—Ç–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)[/]")
        console.print("  [dim]python sdb.py backup create --type=full[/]")

        if db_info["type"] == "sqlite":
            console.print("\n  [green]# –¢–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã (SQLite —É–∂–µ –≤–∫–ª—é—á–µ–Ω)[/]")
            console.print("  [dim]python sdb.py backup create --type=files[/]")

        if db_info["needs_separate_backup"]:
            console.print("\n  [green]# –¢–æ–ª—å–∫–æ –ë–î (PostgreSQL/MySQL)[/]")
            console.print(
                "  [dim]python sdb.py backup create --type=db --db-url=–≤–∞—à_url[/]"
            )

    else:
        console.print("  [yellow]# –ö–∞—Å—Ç–æ–º–Ω—ã–π –±—ç–∫–∞–ø —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—É—Ç–∏[/]")
        console.print(f"  [dim]python sdb.py backup create --type=custom {path}[/]")

        console.print("\n  [green]# –ë—ç–∫–∞–ø –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)[/]")
        console.print("  [dim]python sdb.py backup create --type=full[/]")

    console.print(f"\n[green]‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω[/]")


@backup_app.command("diff")
def diff_backup(
    backup_path: str = typer.Argument(..., help="–ò–º—è –∏–ª–∏ –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"),
    show_details: bool = typer.Option(
        False, "--details", "-d", help="–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö"
    ),
    check_hashes: bool = typer.Option(
        True, "--check-hashes/--no-check-hashes", help="–ü—Ä–æ–≤–µ—Ä—è—Ç—å —Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤"
    ),
    ignore_timestamps: bool = typer.Option(
        False, "--ignore-timestamps", help="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏"
    ),
    exclude: Optional[List[str]] = typer.Option(
        None, "--exclude", "-x", help="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
    ),
):
    """üîç –°—Ä–∞–≤–Ω–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º –±—ç–∫–∞–ø–æ–º."""
    backup_path = _resolve_backup_path(backup_path)
    console.print(f"[bold cyan]üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å –±—ç–∫–∞–ø–æ–º: {backup_path}[/]")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±—ç–∫–∞–ø–∞
    is_archive = backup_path.suffix == ".gz" and backup_path.suffixes[-2:] == [".tar", ".gz"]
    
    if is_archive:
        console.print("[cyan]üì¶ –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞...[/]")
        temp_dir = backup_path.parent / f"temp_diff_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        temp_dir.mkdir(exist_ok=True)
        
        try:
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(temp_dir)
            backup_source = temp_dir
            cleanup_temp = True
        except Exception as e:
            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}[/]")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return
    else:
        console.print("[cyan]üìÅ –ê–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏ —Å –±—ç–∫–∞–ø–æ–º...[/]")
        backup_source = backup_path
        cleanup_temp = False
    
    try:
        # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞
        metadata_file = backup_source / "metadata.json"
        if not metadata_file.exists():
            console.print("[red]‚ùå –§–∞–π–ª metadata.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±—ç–∫–∞–ø–µ[/]")
            return
            
        with open(metadata_file, encoding="utf-8") as f:
            backup_metadata = json.load(f)
        
        console.print(f"[green]üìã –¢–∏–ø –±—ç–∫–∞–ø–∞: {backup_metadata.get('type', 'unknown')}[/]")
        console.print(f"[green]üìÖ –°–æ–∑–¥–∞–Ω: {backup_metadata.get('timestamp', 'unknown')}[/]")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –±—ç–∫–∞–ø–∞
        backup_excludes = backup_metadata.get("excluded_patterns", [])
        if exclude:
            backup_excludes.extend(exclude)
        
        # === –°–†–ê–í–ù–ï–ù–ò–ï –§–ê–ô–õ–û–í ===
        if backup_metadata.get("includes_files", False):
            console.print(f"\n[cyan]üìÅ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...[/]")
            
            # –ß–∏—Ç–∞–µ–º —Ö–µ—à–∏ –∏–∑ –±—ç–∫–∞–ø–∞
            backup_hashes = {}
            hash_file = backup_source / "file_hashes.json"
            if hash_file.exists():
                with open(hash_file, encoding="utf-8") as f:
                    backup_hashes = json.load(f)
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
            project_root = Path(__file__).resolve().parent.parent
            console.print("[cyan]üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è...[/]")
            
            if check_hashes and backup_hashes:
                current_hashes = scan_directory(project_root, excludes=backup_excludes)
            else:
                # –°–∫–∞–Ω–∏—Ä—É–µ–º –±–µ–∑ —Ö–µ—à–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                current_hashes = {}
                import fnmatch
                
                for file in project_root.rglob("*"):
                    if file.is_file():
                        rel_path = file.relative_to(project_root).as_posix()
                        
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                        excluded = False
                        for excl in backup_excludes:
                            if "*" in excl:
                                if fnmatch.fnmatch(rel_path, excl) or fnmatch.fnmatch(file.name, excl):
                                    excluded = True
                                    break
                            elif "/" in excl:
                                if rel_path.startswith(excl) or rel_path == excl:
                                    excluded = True
                                    break
                            else:
                                path_parts = rel_path.split("/")
                                if excl in path_parts or excl == file.name:
                                    excluded = True
                                    break
                        
                        if not excluded:
                            current_hashes[rel_path] = ""  # –ü—É—Å—Ç–æ–π —Ö–µ—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
            added_files = []
            deleted_files = []
            modified_files = []
            
            # –§–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º–µ, –Ω–æ –Ω–µ –≤ –±—ç–∫–∞–ø–µ (–¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ)
            for file_path in current_hashes:
                if file_path not in backup_hashes:
                    file_stat = (project_root / file_path).stat()
                    added_files.append({
                        "path": file_path,
                        "size": file_stat.st_size,
                        "modified": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                    })
            
            # –§–∞–π–ª—ã –≤ –±—ç–∫–∞–ø–µ, –Ω–æ –Ω–µ –≤ —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º–µ (—É–¥–∞–ª—ë–Ω–Ω—ã–µ)
            for file_path in backup_hashes:
                if file_path not in current_hashes:
                    deleted_files.append({
                        "path": file_path,
                        "hash": backup_hashes[file_path][:8] if backup_hashes[file_path] else "no-hash",
                    })
            
            # –§–∞–π–ª—ã –≤ –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ)
            for file_path in backup_hashes:
                if file_path in current_hashes:
                    current_file = project_root / file_path
                    if current_file.exists():
                        backup_hash = backup_hashes[file_path]
                        current_hash = current_hashes[file_path]
                        
                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ö–µ—à–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
                        if check_hashes and backup_hash and current_hash:
                            if backup_hash != current_hash:
                                file_stat = current_file.stat()
                                modified_files.append({
                                    "path": file_path,
                                    "backup_hash": backup_hash[:8],
                                    "current_hash": current_hash[:8],
                                    "size": file_stat.st_size,
                                    "modified": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                                    "reason": "hash_different"
                                })
                        elif not check_hashes:
                            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            file_stat = current_file.stat()
                            backup_files_dir = backup_source / FILES_BACKUP_DIR_NAME / file_path
                            
                            if backup_files_dir.exists():
                                backup_stat = backup_files_dir.stat()
                                size_different = file_stat.st_size != backup_stat.st_size
                                
                                if not ignore_timestamps:
                                    time_different = abs(file_stat.st_mtime - backup_stat.st_mtime) > 2
                                else:
                                    time_different = False
                                
                                if size_different or time_different:
                                    reasons = []
                                    if size_different:
                                        reasons.append("size_different")
                                    if time_different:
                                        reasons.append("time_different")
                                    
                                    modified_files.append({
                                        "path": file_path,
                                        "backup_size": backup_stat.st_size,
                                        "current_size": file_stat.st_size,
                                        "modified": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                                        "reason": "+".join(reasons)
                                    })
            
            # === –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            console.print(f"\n[bold green]üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:[/]")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_table = Table(title="üìà –°–≤–æ–¥–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
            stats_table.add_column("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", style="bold")
            stats_table.add_column("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", style="cyan")
            stats_table.add_column("–û–ø–∏—Å–∞–Ω–∏–µ", style="dim")
            
            stats_table.add_row("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ", f"{len(added_files)}", "–ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ —Å–∏—Å—Ç–µ–º–µ")
            stats_table.add_row("‚ùå –£–¥–∞–ª–µ–Ω–æ", f"{len(deleted_files)}", "–§–∞–π–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–∏—Å—Ç–µ–º–µ")
            stats_table.add_row("üîÑ –ò–∑–º–µ–Ω–µ–Ω–æ", f"{len(modified_files)}", "–§–∞–π–ª—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
            
            console.print(stats_table)
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞
            if show_details:
                if added_files:
                    console.print(f"\n[bold green]‚úÖ –î–û–ë–ê–í–õ–ï–ù–ù–´–ï –§–ê–ô–õ–´ ({len(added_files)}):[/]")
                    added_table = Table()
                    added_table.add_column("–§–∞–π–ª", style="green")
                    added_table.add_column("–†–∞–∑–º–µ—Ä", style="cyan")
                    added_table.add_column("–ò–∑–º–µ–Ω—ë–Ω", style="dim")
                    
                    for file_info in added_files[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
                        size_mb = file_info["size"] / (1024 * 1024) if file_info["size"] > 1024*1024 else file_info["size"]
                        size_str = f"{size_mb:.1f}MB" if file_info["size"] > 1024*1024 else f"{file_info['size']}B"
                        added_table.add_row(
                            file_info["path"],
                            size_str,
                            file_info["modified"][:16]
                        )
                    
                    console.print(added_table)
                    if len(added_files) > 20:
                        console.print(f"[dim]... –∏ –µ—â—ë {len(added_files) - 20} —Ñ–∞–π–ª–æ–≤[/]")
                
                if deleted_files:
                    console.print(f"\n[bold red]‚ùå –£–î–ê–õ–Å–ù–ù–´–ï –§–ê–ô–õ–´ ({len(deleted_files)}):[/]")
                    deleted_table = Table()
                    deleted_table.add_column("–§–∞–π–ª", style="red")
                    deleted_table.add_column("–•–µ—à (–±—ç–∫–∞–ø)", style="dim")
                    
                    for file_info in deleted_files[:20]:
                        deleted_table.add_row(
                            file_info["path"],
                            file_info["hash"]
                        )
                    
                    console.print(deleted_table)
                    if len(deleted_files) > 20:
                        console.print(f"[dim]... –∏ –µ—â—ë {len(deleted_files) - 20} —Ñ–∞–π–ª–æ–≤[/]")
                
                if modified_files:
                    console.print(f"\n[bold yellow]üîÑ –ò–ó–ú–ï–ù–Å–ù–ù–´–ï –§–ê–ô–õ–´ ({len(modified_files)}):[/]")
                    modified_table = Table()
                    modified_table.add_column("–§–∞–π–ª", style="yellow")
                    modified_table.add_column("–ü—Ä–∏—á–∏–Ω–∞", style="magenta")
                    
                    if check_hashes:
                        modified_table.add_column("–•–µ—à (–±—ç–∫–∞–ø)", style="dim")
                        modified_table.add_column("–•–µ—à (—Ç–µ–∫—É—â–∏–π)", style="cyan")
                    else:
                        modified_table.add_column("–†–∞–∑–º–µ—Ä (–±—ç–∫–∞–ø)", style="dim")
                        modified_table.add_column("–†–∞–∑–º–µ—Ä (—Ç–µ–∫—É—â–∏–π)", style="cyan")
                    
                    for file_info in modified_files[:20]:
                        if check_hashes:
                            modified_table.add_row(
                                file_info["path"],
                                file_info["reason"],
                                file_info.get("backup_hash", "N/A"),
                                file_info.get("current_hash", "N/A")
                            )
                        else:
                            backup_size = file_info.get("backup_size", 0)
                            current_size = file_info.get("current_size", 0)
                            backup_size_str = f"{backup_size}B" if backup_size < 1024*1024 else f"{backup_size/(1024*1024):.1f}MB"
                            current_size_str = f"{current_size}B" if current_size < 1024*1024 else f"{current_size/(1024*1024):.1f}MB"
                            
                            modified_table.add_row(
                                file_info["path"],
                                file_info["reason"],
                                backup_size_str,
                                current_size_str
                            )
                    
                    console.print(modified_table)
                    if len(modified_files) > 20:
                        console.print(f"[dim]... –∏ –µ—â—ë {len(modified_files) - 20} —Ñ–∞–π–ª–æ–≤[/]")
            else:
                # –ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if added_files:
                    console.print(f"[green]‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(added_files)} (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --details –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π)[/]")
                if deleted_files:
                    console.print(f"[red]‚ùå –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(deleted_files)} (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --details –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π)[/]")
                if modified_files:
                    console.print(f"[yellow]üîÑ –ò–∑–º–µ–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(modified_files)} (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --details –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π)[/]")
            
            # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            total_changes = len(added_files) + len(deleted_files) + len(modified_files)
            if total_changes == 0:
                console.print(f"\n[bold green]üéâ –°–∏—Å—Ç–µ–º–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–∞ –±—ç–∫–∞–ø—É![/]")
            else:
                console.print(f"\n[bold yellow]‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_changes} –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–µ[/]")
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                if len(added_files) > 10 or len(modified_files) > 10:
                    console.print(f"[blue]üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π –±—ç–∫–∞–ø –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π[/]")
                    console.print(f"[dim]   python3 sdb.py backup create --type=full[/]")
        
        else:
            console.print("[yellow]‚ö†Ô∏è –ë—ç–∫–∞–ø –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è[/]")
        
        # === –°–†–ê–í–ù–ï–ù–ò–ï –ë–ê–ó–´ –î–ê–ù–ù–´–• ===
        if backup_metadata.get("includes_db", False):
            console.print(f"\n[cyan]üóÉÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...[/]")
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ë–î
            backup_db = backup_metadata.get("database", {})
            current_db = detect_database_type()
            
            console.print(f"[dim]–¢–∏–ø –ë–î –≤ –±—ç–∫–∞–ø–µ: {backup_db.get('type', 'unknown')}[/]")
            console.print(f"[dim]–¢–µ–∫—É—â–∏–π —Ç–∏–ø –ë–î: {current_db.get('type', 'unknown')}[/]")
            
            if backup_db.get("type") != current_db.get("type"):
                console.print("[yellow]‚ö†Ô∏è –¢–∏–ø –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–∏–ª—Å—è![/]")
            
            if current_db.get("type") == "sqlite":
                # –î–ª—è SQLite —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ –ë–î
                current_db_files = current_db.get("detected_files", [])
                backup_db_files = backup_db.get("files", [])
                
                if set(current_db_files) != set(backup_db_files):
                    console.print("[yellow]üîÑ –§–∞–π–ª—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–∏–ª–∏—Å—å[/]")
                    console.print(f"   –ë—ç–∫–∞–ø: {backup_db_files}")
                    console.print(f"   –¢–µ–∫—É—â–∏–µ: {current_db_files}")
                else:
                    console.print("[green]‚úÖ –§–∞–π–ª—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –±—ç–∫–∞–ø—É[/]")
            else:
                console.print("[blue]üí° –î–ª—è –≤–Ω–µ—à–Ω–∏—Ö –ë–î —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–º–ø–æ–≤[/]")
        
    finally:
        if cleanup_temp:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    console.print(f"\n[green]‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ[/]")

def _resolve_backup_path(backup_path: Union[str, Path]) -> Path:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É/–∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –ø—É—Ç–∏."""
    p = Path(backup_path)
    if p.exists():
        return p.resolve()
    # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç / ‚Äî –∏—â–µ–º –≤ backup/
    if not p.is_absolute() and '/' not in str(p):
        backup_dir = _get_backup_base_dir()
        candidate = backup_dir / p
        if candidate.exists():
            return candidate.resolve()
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å
    return p.resolve()

# --- –ö–æ–Ω–µ—Ü backup_unified.py ---
